{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_utils import call_openai_chat_completion\n",
    "from jinja2 import Environment\n",
    "from textwrap import dedent\n",
    "import json\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/yinhong/.cache/huggingface/datasets/json/default-488211ad4c96df17/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de754abff9384c14b9dd2e27fcc5bac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news_data = load_dataset('json', data_files=\"news_annotations.jsonl\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 14/110 [00:17<01:43,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too long?  Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, you requested 4171 tokens (3915 in the messages, 256 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 40/110 [00:53<01:38,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too long?  Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4156 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 48/110 [01:03<01:06,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too long?  Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4296 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 77/110 [01:39<00:30,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too long?  Error code: 400 - {'error': {'message': \"This model's maximum context length is 4097 tokens. However, your messages resulted in 4311 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:21<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluation_prompt = dedent(\"\"\"\\\n",
    "    Please pretend you are a human reader. Read the tree versions of the news article below and rate their coherence scores following the guideline.\n",
    "    Coherence guidelines:\n",
    "    1. Evaluate how well the sentences transition from one to another. A fluent text should have seamless connections between sentences.\n",
    "    2. Evaluate how well the sentences are organized and the ideas are conveyed. A coherent text should have a clear and precise structure.\n",
    "    General guidelines:\n",
    "    1. Rate the coherence of the text from 1 to 10, where 1 is the lowest and 10 is the highest.\n",
    "    2. Utilize the entire rating scale, from the lowest to the highest score, to provide nuanced feedback.\n",
    "    3. Please return in JSON format. For example, {\"score1\": 1, \"score2\": 2, \"score3\": 3}.\n",
    "\n",
    "    News headline:\n",
    "    {{ headline }}\n",
    "\n",
    "    Version 1:\n",
    "    {{ version1 }}\n",
    "\n",
    "    Version 2:\n",
    "    {{ version2 }}\n",
    "\n",
    "    Version 3:\n",
    "    {{ version3 }}\n",
    "\n",
    "    Rating:\\\n",
    "\"\"\")\n",
    "\n",
    "environment = Environment()\n",
    "evaluation_prompt = environment.from_string(evaluation_prompt)\n",
    "\n",
    "gpt_scores = []\n",
    "for id in tqdm(range(news_data.shape[0])):\n",
    "    try:\n",
    "        prompt = evaluation_prompt.render(\n",
    "            headline=news_data['input'][id],\n",
    "            version1=news_data['output_shuffled_0'][id],\n",
    "            version2=news_data['output_shuffled_1'][id],\n",
    "            version3=news_data['output_shuffled_2'][id]\n",
    "        )\n",
    "        random_indice = news_data['random_indice'][id]\n",
    "        random_indice = ast.literal_eval(random_indice)\n",
    "        llm_output = call_openai_chat_completion(prompt)\n",
    "        corrected_score = [list(llm_output.values())[i] for i in random_indice]\n",
    "        gpt_scores.append(corrected_score)\n",
    "    except Exception as e:\n",
    "        print('Too long? ', e)\n",
    "        gpt_scores.append(None)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "news_data = news_data.add_column(\"gpt_scores\", gpt_scores)\n",
    "news_data.to_json(\"news_annotations.jsonl\", orient=\"records\", lines=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6095238095238096"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = [s for s in gpt_scores if s!=[]]\n",
    "\n",
    "sum([1 if s[1]<s[2] else 0 for s in gs]) / len(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/yinhong/.cache/huggingface/datasets/json/default-9ebebbe1ac6918db/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b90f3d5a5d64eef9f5908ca86de7dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29278023360147a58d320b3eb62c8dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7b08accdfa40f8b8aadd2253002e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/yinhong/.cache/huggingface/datasets/json/default-9ebebbe1ac6918db/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2ea1739ba34dab91be02d058d4b952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lfqa_data = load_dataset('json', data_files=\"lfqa_annotations.jsonl\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [07:04<00:00,  3.86s/it] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluation_prompt = dedent(\"\"\"\\\n",
    "    Please pretend you are a human reader. Read the tree versions of the answers below for a given question and rate their coherence scores following the guideline.\n",
    "    Coherence guidelines:\n",
    "    1. Evaluate how well the sentences transition from one to another. A fluent text should have seamless connections between sentences.\n",
    "    2. Evaluate how well the sentences are organized and the ideas are conveyed. A coherent text should have a clear and precise structure.\n",
    "    General guidelines:\n",
    "    1. Rate the coherence of the text from 1 to 10, where 1 is the lowest and 10 is the highest.\n",
    "    2. Utilize the entire rating scale, from the lowest to the highest score, to provide nuanced feedback.\n",
    "    3. Please return in JSON format. For example, {\"score1\": 1, \"score2\": 2, \"score3\": 3}.\n",
    "\n",
    "    Question:\n",
    "    {{ headline }}\n",
    "\n",
    "    Answer version 1:\n",
    "    {{ version1 }}\n",
    "\n",
    "    Answer version 2:\n",
    "    {{ version2 }}\n",
    "\n",
    "    Answer version 3:\n",
    "    {{ version3 }}\n",
    "\n",
    "    Rating:\\\n",
    "\"\"\")\n",
    "\n",
    "environment = Environment()\n",
    "evaluation_prompt = environment.from_string(evaluation_prompt)\n",
    "\n",
    "gpt_scores = []\n",
    "for id in tqdm(range(lfqa_data.shape[0])):\n",
    "    try:\n",
    "        prompt = evaluation_prompt.render(\n",
    "            headline=lfqa_data['question'][id],\n",
    "            version1=lfqa_data['answer_shuffled_0'][id],\n",
    "            version2=lfqa_data['answer_shuffled_1'][id],\n",
    "            version3=lfqa_data['answer_shuffled_2'][id]\n",
    "        )\n",
    "        random_indice = lfqa_data['random_indice'][id]\n",
    "        random_indice = ast.literal_eval(random_indice)\n",
    "        llm_output = call_openai_chat_completion(prompt)\n",
    "        corrected_score = [list(llm_output.values())[i] for i in random_indice]\n",
    "        gpt_scores.append(corrected_score)\n",
    "    except Exception as e:\n",
    "        print('Too long? ',e)\n",
    "        gpt_scores.append(None)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a12bf8228514db1a54bf595dcb621c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "775177"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the data\n",
    "lfqa_data = lfqa_data.add_column(\"gpt_scores\", gpt_scores)\n",
    "lfqa_data.to_json(\"lfqa_annotations.jsonl\", orient=\"records\", lines=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5818181818181818"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = [s for s in gpt_scores if s!=[]]\n",
    "\n",
    "sum([1 if s[1]<s[2] else 0 for s in gs]) / len(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/yinhong/.cache/huggingface/datasets/json/default-4e8a2b4466867fad/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3864cd4084364dcbb18bae7a2b54550b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e534f0d85bc4af182f5ed68e2af2c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bafd19e99e647f1a328de4e60f4f678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/yinhong/.cache/huggingface/datasets/json/default-4e8a2b4466867fad/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed709c7e62c490aaf27b3e4229673a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "recipe_data = load_dataset('json', data_files=\"recipe_annotations.jsonl\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:30<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluation_prompt = dedent(\"\"\"\\\n",
    "    Please pretend you are a human reader. Read the tree versions of the recipes below for a given dish title and rate their coherence scores following the guideline.\n",
    "    Coherence guidelines:\n",
    "    1. Evaluate how well the sentences transition from one to another. A fluent text should have seamless connections between sentences.\n",
    "    2. Evaluate how well the sentences are organized and the ideas are conveyed. A coherent text should have a clear and precise structure.\n",
    "    General guidelines:\n",
    "    1. Rate the coherence of the text from 1 to 10, where 1 is the lowest and 10 is the highest.\n",
    "    2. Utilize the entire rating scale, from the lowest to the highest score, to provide nuanced feedback.\n",
    "    3. Please return in JSON format. For example, {\"score1\": 1, \"score2\": 2, \"score3\": 3}.\n",
    "\n",
    "    Dish title:\n",
    "    {{ title }}\n",
    "\n",
    "    Recipe version 1:\n",
    "    {{ version1 }}\n",
    "\n",
    "    Recipe version 2:\n",
    "    {{ version2 }}\n",
    "\n",
    "    Recipe version 3:\n",
    "    {{ version3 }}\n",
    "\n",
    "    Rating:\\\n",
    "\"\"\")\n",
    "\n",
    "environment = Environment()\n",
    "evaluation_prompt = environment.from_string(evaluation_prompt)\n",
    "\n",
    "gpt_scores = []\n",
    "for id in tqdm(range(recipe_data.shape[0])):\n",
    "    try:\n",
    "        prompt = evaluation_prompt.render(\n",
    "            title=recipe_data['title'][id],\n",
    "            version1=recipe_data['answer_shuffled_0'][id],\n",
    "            version2=recipe_data['answer_shuffled_1'][id],\n",
    "            version3=recipe_data['answer_shuffled_2'][id]\n",
    "        )\n",
    "        random_indice = lfqa_data['random_indice'][id]\n",
    "        random_indice = ast.literal_eval(random_indice)\n",
    "        llm_output = call_openai_chat_completion(prompt)\n",
    "        corrected_score = [list(llm_output.values())[i] for i in random_indice]\n",
    "        gpt_scores.append(corrected_score)\n",
    "    except Exception as e:\n",
    "        print('Too long? ',e)\n",
    "        gpt_scores.append(None)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb311f17120e47328e72790bff96bdb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "484996"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save data\n",
    "recipe_data = recipe_data.add_column(\"gpt_scores\", gpt_scores)\n",
    "recipe_data.to_json(\"recipe_annotations.jsonl\", orient=\"records\", lines=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5363636363636364"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = [s for s in gpt_scores if s!=[]]\n",
    "\n",
    "sum([1 if s[1]<s[2] else 0 for s in gs]) / len(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
